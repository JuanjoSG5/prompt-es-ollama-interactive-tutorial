{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial How-To\n",
    "\n",
    "Este tutorial **requiere de Ollama** para la interaccion. Si no tienes Ollama instalado, es facil, descargalo e instalalo desde aqui [Instalacion de Ollama](https://ollama.com/download).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Como empezar\n",
    "\n",
    "1. Clona este repositorio en tu ordenador\n",
    "\n",
    "2. Instala las dependencias con los siguientes comandos\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U ollama;\n",
    "%pip install pickleshare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Dale un nombre al modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"qwen2.5:14b\"\n",
    "\n",
    "# Guardar el nombre del modelo para usarlo en el resto de tutoriales\n",
    "%store MODEL_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Ejecuta las celdas de codigo en orden, siguiendo las instrucciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notas de uso y tipsðŸ’¡\n",
    "\n",
    "- Este curso usa [Qwen 2.5 14B](https://ollama.com/library/qwen2.5:14b) con una temperatura de 0. MÃ¡s adelante explicare que es la temperatura. Por ahora, es suficiente con saber que esta configuracion da resultados mÃ¡s deterministicos. Todas las tecnicas de prompt engineering en este curso se aplican tambien al resto de modelos disponibles en  [Ollama model library](https://ollama.com/search).\n",
    "\n",
    "- Puedes usar `Shift + Enter` para ejecutar una celda e ir a la siguiente.\n",
    "\n",
    "- Cuando llegues al final de la pagina, continua con el sguiente archivo enumerado, o a la siguiente carpeta si ya has terminado con el contenido del capitulo.\n",
    "\n",
    "### La libreria Ollama en Python \n",
    "Vamos a usar la libreria: [Ollama Python Library](https://github.com/ollama/ollama-python) durante este tutorial. \n",
    "\n",
    "Debajo hay un ejemplo de un prompt en este tutorial. Primero, creamos `get_completion`, que es una funcion auciliar que manda el promot a Ollama y devuelve la repuesta generada por Ollama. Prueba a ejecutarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat, ChatResponse, Options\n",
    "\n",
    "def get_completion(prompt: str):\n",
    "    response = chat(\n",
    "        model=MODEL_NAME,\n",
    "        options=Options(\n",
    "            max_tokens=2000,\n",
    "            temperature=0.0,\n",
    "        ),\n",
    "        messages=[\n",
    "          {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a descargar el modelo primero, vamos a usar la variable MODEL_NAME  que definimos antes para determinar el modelo que se usaremos durante este tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from ollama import pull\n",
    "\n",
    "\n",
    "current_digest, bars = '', {}\n",
    "for progress in pull(MODEL_NAME, stream=True):\n",
    "  digest = progress.get('digest', '')\n",
    "  if digest != current_digest and current_digest in bars:\n",
    "    bars[current_digest].close()\n",
    "\n",
    "  if not digest:\n",
    "    print(progress.get('status'))\n",
    "    continue\n",
    "\n",
    "  if digest not in bars and (total := progress.get('total')):\n",
    "    bars[digest] = tqdm(total=total, desc=f'pulling {digest[7:19]}', unit='B', unit_scale=True)\n",
    "\n",
    "  if completed := progress.get('completed'):\n",
    "    bars[digest].update(completed - bars[digest].n)\n",
    "\n",
    "  current_digest = digest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now we will write out an example prompt for Ollama and print Ollama's output by running our `get_completion` helper function. Running the cell below will print out a response from Ollama beneath it.\n",
    "\n",
    "Feel free to play around with the prompt string to elicit different responses from Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt = \"Hola! Como estas?\"\n",
    "\n",
    "# Recibimos la respuesta de Ollama\n",
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `MODEL_NAME` variables define earlier will be used throughout the tutorial. Just make sure to run the cells for each tutorial page from top to bottom."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
